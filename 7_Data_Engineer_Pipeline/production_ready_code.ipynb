{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ae911-8e57-4558-80e3-5b30737e2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 1. AIRFLOW DAG EXAMPLE\n",
    "# ====================\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "def extract_data(**context):\n",
    "    \"\"\"Extract data from source system\"\"\"\n",
    "    import pandas as pd\n",
    "    # Simulate data extraction\n",
    "    data = pd.DataFrame({\n",
    "        'id': range(1, 101),\n",
    "        'value': range(100, 200),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=100, freq='H')\n",
    "    })\n",
    "    # Save to temporary location\n",
    "    data.to_parquet('/tmp/extracted_data.parquet')\n",
    "    return '/tmp/extracted_data.parquet'\n",
    "\n",
    "def transform_data(**context):\n",
    "    \"\"\"Transform extracted data\"\"\"\n",
    "    import pandas as pd\n",
    "    file_path = context['task_instance'].xcom_pull(task_ids='extract')\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Apply transformations\n",
    "    df['value_squared'] = df['value'] ** 2\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    \n",
    "    transformed_path = '/tmp/transformed_data.parquet'\n",
    "    df.to_parquet(transformed_path)\n",
    "    return transformed_path\n",
    "\n",
    "# Define DAG\n",
    "dag = DAG(\n",
    "    'etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='ETL pipeline example',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    tags=['etl', 'example']\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = PostgresOperator(\n",
    "    task_id='load',\n",
    "    postgres_conn_id='postgres_default',\n",
    "    sql=\"\"\"\n",
    "    COPY target_table FROM '{{ ti.xcom_pull(task_ids='transform') }}'\n",
    "    WITH (FORMAT parquet);\n",
    "    \"\"\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "quality_check = BashOperator(\n",
    "    task_id='data_quality_check',\n",
    "    bash_command='python /opt/airflow/scripts/quality_check.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Set dependencies\n",
    "extract_task >> transform_task >> load_task >> quality_check\n",
    "\n",
    "# ====================\n",
    "# 2. DOCKER DEPLOYMENT\n",
    "# ====================\n",
    "\n",
    "# Dockerfile\n",
    "\"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY src/ ./src/\n",
    "COPY config/ ./config/\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"python\", \"src/main.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# docker-compose.yml\n",
    "\"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  etl-app:\n",
    "    build: .\n",
    "    environment:\n",
    "      - DATABASE_URL=${DATABASE_URL}\n",
    "      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n",
    "      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "      - ./data:/app/data\n",
    "    depends_on:\n",
    "      - postgres\n",
    "      - redis\n",
    "    networks:\n",
    "      - etl-network\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      POSTGRES_DB: etl_db\n",
    "      POSTGRES_USER: etl_user\n",
    "      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    networks:\n",
    "      - etl-network\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    networks:\n",
    "      - etl-network\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "\n",
    "networks:\n",
    "  etl-network:\n",
    "    driver: bridge\n",
    "\"\"\"\n",
    "\n",
    "# ====================\n",
    "# 3. CLOUD STORAGE (S3/GCS)\n",
    "# ====================\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "class CloudStorageManager:\n",
    "    def __init__(self):\n",
    "        # AWS S3 setup\n",
    "        self.s3_client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "            region_name=os.getenv('AWS_REGION', 'us-east-1')\n",
    "        )\n",
    "        \n",
    "        # Google Cloud Storage setup\n",
    "        self.gcs_client = storage.Client()\n",
    "    \n",
    "    def upload_to_s3(self, dataframe, bucket_name, key):\n",
    "        \"\"\"Upload DataFrame to S3 as parquet\"\"\"\n",
    "        buffer = BytesIO()\n",
    "        dataframe.to_parquet(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        self.s3_client.upload_fileobj(\n",
    "            buffer, \n",
    "            bucket_name, \n",
    "            key,\n",
    "            ExtraArgs={'ContentType': 'application/octet-stream'}\n",
    "        )\n",
    "        print(f\"Uploaded to s3://{bucket_name}/{key}\")\n",
    "    \n",
    "    def download_from_s3(self, bucket_name, key):\n",
    "        \"\"\"Download parquet file from S3 to DataFrame\"\"\"\n",
    "        response = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        return pd.read_parquet(BytesIO(response['Body'].read()))\n",
    "    \n",
    "    def upload_to_gcs(self, dataframe, bucket_name, blob_name):\n",
    "        \"\"\"Upload DataFrame to Google Cloud Storage\"\"\"\n",
    "        bucket = self.gcs_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        buffer = BytesIO()\n",
    "        dataframe.to_parquet(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        blob.upload_from_file(buffer, content_type='application/octet-stream')\n",
    "        print(f\"Uploaded to gs://{bucket_name}/{blob_name}\")\n",
    "    \n",
    "    def download_from_gcs(self, bucket_name, blob_name):\n",
    "        \"\"\"Download parquet file from GCS to DataFrame\"\"\"\n",
    "        bucket = self.gcs_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        buffer = BytesIO()\n",
    "        blob.download_to_file(buffer)\n",
    "        buffer.seek(0)\n",
    "        return pd.read_parquet(buffer)\n",
    "\n",
    "# Usage example\n",
    "storage_manager = CloudStorageManager()\n",
    "\n",
    "# Create sample data\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=1000),\n",
    "    'sales': range(1000, 2000),\n",
    "    'region': ['US', 'EU', 'ASIA'] * 334\n",
    "})\n",
    "\n",
    "# Upload to both S3 and GCS\n",
    "storage_manager.upload_to_s3(df, 'my-data-bucket', 'sales/2024/data.parquet')\n",
    "storage_manager.upload_to_gcs(df, 'my-gcs-bucket', 'sales/2024/data.parquet')\n",
    "\n",
    "# ====================\n",
    "# 4. DATA WAREHOUSE CONNECTIONS\n",
    "# ====================\n",
    "\n",
    "import snowflake.connector\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "class DataWarehouseConnector:\n",
    "    def __init__(self):\n",
    "        self.snowflake_conn = None\n",
    "        self.bigquery_client = None\n",
    "        self.setup_connections()\n",
    "    \n",
    "    def setup_connections(self):\n",
    "        # Snowflake connection\n",
    "        self.snowflake_conn = snowflake.connector.connect(\n",
    "            user=os.getenv('SNOWFLAKE_USER'),\n",
    "            password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "            account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "            database=os.getenv('SNOWFLAKE_DATABASE'),\n",
    "            schema=os.getenv('SNOWFLAKE_SCHEMA')\n",
    "        )\n",
    "        \n",
    "        # BigQuery client\n",
    "        self.bigquery_client = bigquery.Client(\n",
    "            project=os.getenv('GCP_PROJECT_ID')\n",
    "        )\n",
    "    \n",
    "    def execute_snowflake_query(self, query):\n",
    "        \"\"\"Execute query on Snowflake and return DataFrame\"\"\"\n",
    "        cursor = self.snowflake_conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            return pd.DataFrame(data, columns=columns)\n",
    "        finally:\n",
    "            cursor.close()\n",
    "    \n",
    "    def load_to_snowflake(self, dataframe, table_name, if_exists='append'):\n",
    "        \"\"\"Load DataFrame to Snowflake table\"\"\"\n",
    "        engine = create_engine(\n",
    "            f\"snowflake://{os.getenv('SNOWFLAKE_USER')}:{os.getenv('SNOWFLAKE_PASSWORD')}\"\n",
    "            f\"@{os.getenv('SNOWFLAKE_ACCOUNT')}/{os.getenv('SNOWFLAKE_DATABASE')}\"\n",
    "            f\"/{os.getenv('SNOWFLAKE_SCHEMA')}?warehouse={os.getenv('SNOWFLAKE_WAREHOUSE')}\"\n",
    "        )\n",
    "        \n",
    "        dataframe.to_sql(\n",
    "            table_name, \n",
    "            engine, \n",
    "            if_exists=if_exists, \n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "    \n",
    "    def execute_bigquery_query(self, query):\n",
    "        \"\"\"Execute query on BigQuery and return DataFrame\"\"\"\n",
    "        query_job = self.bigquery_client.query(query)\n",
    "        return query_job.to_dataframe()\n",
    "    \n",
    "    def load_to_bigquery(self, dataframe, table_id, if_exists='append'):\n",
    "        \"\"\"Load DataFrame to BigQuery table\"\"\"\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        \n",
    "        if if_exists == 'replace':\n",
    "            job_config.write_disposition = 'WRITE_TRUNCATE'\n",
    "        else:\n",
    "            job_config.write_disposition = 'WRITE_APPEND'\n",
    "        \n",
    "        job_config.autodetect = True\n",
    "        \n",
    "        job = self.bigquery_client.load_table_from_dataframe(\n",
    "            dataframe, table_id, job_config=job_config\n",
    "        )\n",
    "        job.result()  # Wait for job to complete\n",
    "\n",
    "# Usage example\n",
    "dw = DataWarehouseConnector()\n",
    "\n",
    "# Query from Snowflake\n",
    "sf_data = dw.execute_snowflake_query(\"SELECT * FROM sales_data WHERE date >= '2024-01-01'\")\n",
    "\n",
    "# Query from BigQuery\n",
    "bq_data = dw.execute_bigquery_query(\"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        SUM(revenue) as total_revenue\n",
    "    FROM `project.dataset.sales`\n",
    "    WHERE date >= '2024-01-01'\n",
    "    GROUP BY product_id\n",
    "\"\"\")\n",
    "\n",
    "# ====================\n",
    "# 5. SECRETS MANAGEMENT\n",
    "# ====================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from google.cloud import secretmanager\n",
    "import hvac  # HashiCorp Vault\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "class SecretsManager:\n",
    "    def __init__(self):\n",
    "        self.aws_secrets = boto3.client('secretsmanager')\n",
    "        self.gcp_secrets = secretmanager.SecretManagerServiceClient()\n",
    "        self.vault_client = None\n",
    "        self.setup_vault()\n",
    "    \n",
    "    def setup_vault(self):\n",
    "        \"\"\"Setup HashiCorp Vault client\"\"\"\n",
    "        if os.getenv('VAULT_URL'):\n",
    "            self.vault_client = hvac.Client(url=os.getenv('VAULT_URL'))\n",
    "            self.vault_client.token = os.getenv('VAULT_TOKEN')\n",
    "    \n",
    "    def get_aws_secret(self, secret_name, region='us-east-1'):\n",
    "        \"\"\"Retrieve secret from AWS Secrets Manager\"\"\"\n",
    "        try:\n",
    "            response = self.aws_secrets.get_secret_value(\n",
    "                SecretId=secret_name,\n",
    "                VersionStage='AWSCURRENT'\n",
    "            )\n",
    "            return response['SecretString']\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving AWS secret: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_gcp_secret(self, project_id, secret_id, version_id='latest'):\n",
    "        \"\"\"Retrieve secret from Google Secret Manager\"\"\"\n",
    "        try:\n",
    "            name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "            response = self.gcp_secrets.access_secret_version(request={\"name\": name})\n",
    "            return response.payload.data.decode(\"UTF-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving GCP secret: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_vault_secret(self, path):\n",
    "        \"\"\"Retrieve secret from HashiCorp Vault\"\"\"\n",
    "        if not self.vault_client:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            response = self.vault_client.secrets.kv.v2.read_secret_version(path=path)\n",
    "            return response['data']['data']\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving Vault secret: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def encrypt_local_secret(self, secret_value, key=None):\n",
    "        \"\"\"Encrypt secret locally using Fernet\"\"\"\n",
    "        if not key:\n",
    "            key = Fernet.generate_key()\n",
    "        \n",
    "        fernet = Fernet(key)\n",
    "        encrypted_secret = fernet.encrypt(secret_value.encode())\n",
    "        return encrypted_secret, key\n",
    "    \n",
    "    def decrypt_local_secret(self, encrypted_secret, key):\n",
    "        \"\"\"Decrypt locally encrypted secret\"\"\"\n",
    "        fernet = Fernet(key)\n",
    "        return fernet.decrypt(encrypted_secret).decode()\n",
    "\n",
    "# Configuration class using secrets\n",
    "class SecureConfig:\n",
    "    def __init__(self):\n",
    "        self.secrets_manager = SecretsManager()\n",
    "        self.load_config()\n",
    "    \n",
    "    def load_config(self):\n",
    "        \"\"\"Load configuration with secrets from various sources\"\"\"\n",
    "        # Try different secret sources in order of preference\n",
    "        self.database_url = (\n",
    "            self.secrets_manager.get_aws_secret('prod/database/url') or\n",
    "            self.secrets_manager.get_gcp_secret('my-project', 'database-url') or\n",
    "            os.getenv('DATABASE_URL')\n",
    "        )\n",
    "        \n",
    "        self.api_keys = {\n",
    "            'external_api': self.secrets_manager.get_vault_secret('api-keys/external'),\n",
    "            'analytics': self.secrets_manager.get_aws_secret('prod/analytics/key')\n",
    "        }\n",
    "\n",
    "# ====================\n",
    "# 6. MONITORING DASHBOARDS\n",
    "# ====================\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "import logging\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "from datadog import initialize, statsd\n",
    "import json\n",
    "\n",
    "# Prometheus metrics\n",
    "JOBS_TOTAL = Counter('etl_jobs_total', 'Total ETL jobs', ['status', 'job_type'])\n",
    "JOB_DURATION = Histogram('etl_job_duration_seconds', 'ETL job duration', ['job_type'])\n",
    "RECORDS_PROCESSED = Counter('etl_records_processed_total', 'Total records processed')\n",
    "SYSTEM_MEMORY = Gauge('system_memory_usage_bytes', 'System memory usage')\n",
    "\n",
    "class ETLMonitor:\n",
    "    def __init__(self, datadog_api_key=None):\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Setup Datadog if API key provided\n",
    "        if datadog_api_key:\n",
    "            initialize(api_key=datadog_api_key)\n",
    "        \n",
    "        # Start Prometheus metrics server\n",
    "        start_http_server(8000)\n",
    "    \n",
    "    def log_job_start(self, job_name, job_type):\n",
    "        \"\"\"Log job start with metrics\"\"\"\n",
    "        self.logger.info(f\"Starting job: {job_name}\")\n",
    "        return time.time()\n",
    "    \n",
    "    def log_job_completion(self, job_name, job_type, start_time, records_count, status='success'):\n",
    "        \"\"\"Log job completion with metrics\"\"\"\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Update Prometheus metrics\n",
    "        JOBS_TOTAL.labels(status=status, job_type=job_type).inc()\n",
    "        JOB_DURATION.labels(job_type=job_type).observe(duration)\n",
    "        RECORDS_PROCESSED.inc(records_count)\n",
    "        \n",
    "        # Send to Datadog\n",
    "        statsd.increment(f'etl.jobs.{status}', tags=[f'job_type:{job_type}'])\n",
    "        statsd.histogram(f'etl.job.duration', duration, tags=[f'job_type:{job_type}'])\n",
    "        statsd.increment('etl.records.processed', records_count)\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Job completed: {job_name}, Duration: {duration:.2f}s, \"\n",
    "            f\"Records: {records_count}, Status: {status}\"\n",
    "        )\n",
    "    \n",
    "    def monitor_system_metrics(self):\n",
    "        \"\"\"Monitor system-level metrics\"\"\"\n",
    "        while True:\n",
    "            # Memory usage\n",
    "            memory = psutil.virtual_memory()\n",
    "            SYSTEM_MEMORY.set(memory.used)\n",
    "            statsd.gauge('system.memory.used', memory.used)\n",
    "            statsd.gauge('system.memory.percent', memory.percent)\n",
    "            \n",
    "            # CPU usage\n",
    "            cpu_percent = psutil.cpu_percent(interval=1)\n",
    "            statsd.gauge('system.cpu.percent', cpu_percent)\n",
    "            \n",
    "            # Disk usage\n",
    "            disk = psutil.disk_usage('/')\n",
    "            statsd.gauge('system.disk.used', disk.used)\n",
    "            statsd.gauge('system.disk.percent', disk.percent)\n",
    "            \n",
    "            time.sleep(60)  # Update every minute\n",
    "    \n",
    "    def create_custom_dashboard_data(self, job_metrics):\n",
    "        \"\"\"Create custom dashboard data structure\"\"\"\n",
    "        dashboard_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'jobs': {\n",
    "                'total_runs': sum(job_metrics.values()),\n",
    "                'success_rate': job_metrics.get('success', 0) / sum(job_metrics.values()) * 100,\n",
    "                'failure_rate': job_metrics.get('failure', 0) / sum(job_metrics.values()) * 100\n",
    "            },\n",
    "            'system': {\n",
    "                'memory_usage_percent': psutil.virtual_memory().percent,\n",
    "                'cpu_usage_percent': psutil.cpu_percent(),\n",
    "                'disk_usage_percent': psutil.disk_usage('/').percent\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file for custom dashboard consumption\n",
    "        with open('/tmp/etl_metrics.json', 'w') as f:\n",
    "            json.dump(dashboard_data, f)\n",
    "        \n",
    "        return dashboard_data\n",
    "\n",
    "# Usage examples\n",
    "monitor = ETLMonitor(datadog_api_key=os.getenv('DATADOG_API_KEY'))\n",
    "\n",
    "# Monitor a job\n",
    "def sample_etl_job():\n",
    "    start_time = monitor.log_job_start('daily_sales_etl', 'batch')\n",
    "    \n",
    "    try:\n",
    "        # Simulate ETL work\n",
    "        records_processed = 1500\n",
    "        time.sleep(2)  # Simulate processing time\n",
    "        \n",
    "        monitor.log_job_completion(\n",
    "            'daily_sales_etl', \n",
    "            'batch', \n",
    "            start_time, \n",
    "            records_processed, \n",
    "            'success'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        monitor.log_job_completion(\n",
    "            'daily_sales_etl', \n",
    "            'batch', \n",
    "            start_time, \n",
    "            0, \n",
    "            'failure'\n",
    "        )\n",
    "        raise\n",
    "\n",
    "# Run the sample job\n",
    "sample_etl_job()\n",
    "\n",
    "# ====================\n",
    "# REQUIREMENTS.TXT\n",
    "# ====================\n",
    "\n",
    "\"\"\"\n",
    "# Core data processing\n",
    "pandas>=1.5.0\n",
    "numpy>=1.20.0\n",
    "sqlalchemy>=1.4.0\n",
    "\n",
    "# Workflow orchestration\n",
    "apache-airflow>=2.5.0\n",
    "apache-airflow-providers-postgres\n",
    "apache-airflow-providers-amazon\n",
    "apache-airflow-providers-google\n",
    "\n",
    "# Cloud storage\n",
    "boto3>=1.26.0\n",
    "google-cloud-storage>=2.7.0\n",
    "\n",
    "# Data warehouses\n",
    "snowflake-connector-python>=3.0.0\n",
    "google-cloud-bigquery>=3.4.0\n",
    "\n",
    "# Secrets management\n",
    "cryptography>=3.4.0\n",
    "hvac>=1.0.0  # HashiCorp Vault\n",
    "\n",
    "# Monitoring\n",
    "prometheus-client>=0.15.0\n",
    "datadog>=0.44.0\n",
    "psutil>=5.9.0\n",
    "\n",
    "# Development\n",
    "pytest>=7.0.0\n",
    "black>=22.0\n",
    "flake8>=5.0.0\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
