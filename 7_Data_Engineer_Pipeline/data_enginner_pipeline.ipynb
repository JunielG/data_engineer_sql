{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3725673-b0e3-4517-8e4f-9158f8692c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete Data Engineering Pipeline\n",
    "A comprehensive pipeline demonstrating data ingestion, transformation, storage, and monitoring\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib\n",
    "import concurrent.futures\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for the data pipeline\"\"\"\n",
    "    raw_data_path: str = \"data/raw\"\n",
    "    processed_data_path: str = \"data/processed\"\n",
    "    database_path: str = \"data/pipeline.db\"\n",
    "    log_path: str = \"logs/pipeline.log\"\n",
    "    batch_size: int = 1000\n",
    "    max_workers: int = 4\n",
    "    api_rate_limit: int = 100  # requests per minute\n",
    "\n",
    "# Setup logging\n",
    "def setup_logging(log_path: str):\n",
    "    \"\"\"Configure logging for the pipeline\"\"\"\n",
    "    Path(log_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# Data Sources\n",
    "class DataSource(ABC):\n",
    "    \"\"\"Abstract base class for data sources\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def extract(self) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "class APIDataSource(DataSource):\n",
    "    \"\"\"Extract data from REST API\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, endpoints: List[str], headers: Dict[str, str] = None):\n",
    "        self.base_url = base_url\n",
    "        self.endpoints = endpoints\n",
    "        self.headers = headers or {}\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    async def extract(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract data from API endpoints\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        async with aiohttp.ClientSession(headers=self.headers) as session:\n",
    "            for endpoint in self.endpoints:\n",
    "                url = f\"{self.base_url}/{endpoint}\"\n",
    "                try:\n",
    "                    async with session.get(url) as response:\n",
    "                        if response.status == 200:\n",
    "                            data = await response.json()\n",
    "                            if isinstance(data, list):\n",
    "                                all_data.extend(data)\n",
    "                            else:\n",
    "                                all_data.append(data)\n",
    "                            self.logger.info(f\"Extracted {len(data)} records from {endpoint}\")\n",
    "                        else:\n",
    "                            self.logger.error(f\"Failed to fetch {url}: {response.status}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error fetching {url}: {str(e)}\")\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "class FileDataSource(DataSource):\n",
    "    \"\"\"Extract data from files (CSV, JSON, etc.)\"\"\"\n",
    "    \n",
    "    def __init__(self, file_paths: List[str]):\n",
    "        self.file_paths = file_paths\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    async def extract(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract data from files\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for file_path in self.file_paths:\n",
    "            try:\n",
    "                path = Path(file_path)\n",
    "                if path.suffix.lower() == '.csv':\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    all_data.extend(df.to_dict('records'))\n",
    "                elif path.suffix.lower() == '.json':\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        if isinstance(data, list):\n",
    "                            all_data.extend(data)\n",
    "                        else:\n",
    "                            all_data.append(data)\n",
    "                \n",
    "                self.logger.info(f\"Extracted data from {file_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error reading {file_path}: {str(e)}\")\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "# Data Transformers\n",
    "class DataTransformer:\n",
    "    \"\"\"Handle data transformations and cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def clean_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Clean and standardize data\"\"\"\n",
    "        cleaned_data = []\n",
    "        \n",
    "        for record in data:\n",
    "            try:\n",
    "                # Remove null/empty values\n",
    "                cleaned_record = {k: v for k, v in record.items() if v is not None and v != \"\"}\n",
    "                \n",
    "                # Standardize date formats\n",
    "                for key, value in cleaned_record.items():\n",
    "                    if 'date' in key.lower() or 'time' in key.lower():\n",
    "                        cleaned_record[key] = self._standardize_date(value)\n",
    "                \n",
    "                # Add metadata\n",
    "                cleaned_record['_processed_at'] = datetime.now().isoformat()\n",
    "                cleaned_record['_record_hash'] = self._generate_hash(cleaned_record)\n",
    "                \n",
    "                cleaned_data.append(cleaned_record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error cleaning record: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.logger.info(f\"Cleaned {len(cleaned_data)} records\")\n",
    "        return cleaned_data\n",
    "    \n",
    "    def _standardize_date(self, date_value: Any) -> str:\n",
    "        \"\"\"Standardize date formats\"\"\"\n",
    "        if isinstance(date_value, str):\n",
    "            try:\n",
    "                # Try common date formats\n",
    "                for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d-%m-%Y', '%Y-%m-%d %H:%M:%S']:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(date_value, fmt)\n",
    "                        return dt.isoformat()\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "        return str(date_value)\n",
    "    \n",
    "    def _generate_hash(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate hash for record deduplication\"\"\"\n",
    "        # Create a copy without metadata fields\n",
    "        clean_record = {k: v for k, v in record.items() if not k.startswith('_')}\n",
    "        record_str = json.dumps(clean_record, sort_keys=True)\n",
    "        return hashlib.md5(record_str.encode()).hexdigest()\n",
    "    \n",
    "    def aggregate_data(self, data: List[Dict[str, Any]], group_by: str, metrics: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Aggregate data by specified columns\"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if group_by not in df.columns:\n",
    "            self.logger.warning(f\"Group by column '{group_by}' not found\")\n",
    "            return data\n",
    "        \n",
    "        agg_dict = {}\n",
    "        for metric in metrics:\n",
    "            if metric in df.columns and pd.api.types.is_numeric_dtype(df[metric]):\n",
    "                agg_dict[metric] = ['sum', 'mean', 'count']\n",
    "        \n",
    "        if not agg_dict:\n",
    "            self.logger.warning(\"No numeric columns found for aggregation\")\n",
    "            return data\n",
    "        \n",
    "        aggregated = df.groupby(group_by).agg(agg_dict).reset_index()\n",
    "        aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
    "        \n",
    "        result = aggregated.to_dict('records')\n",
    "        self.logger.info(f\"Aggregated data into {len(result)} groups\")\n",
    "        return result\n",
    "\n",
    "# Data Storage\n",
    "class DataStorage:\n",
    "    \"\"\"Handle data storage operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._init_database()\n",
    "    \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize database tables\"\"\"\n",
    "        Path(self.config.database_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with sqlite3.connect(self.config.database_path) as conn:\n",
    "            # Create raw data table\n",
    "            conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS raw_data (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    data_source TEXT NOT NULL,\n",
    "                    record_hash TEXT UNIQUE,\n",
    "                    data JSON NOT NULL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create processed data table\n",
    "            conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS processed_data (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    batch_id TEXT NOT NULL,\n",
    "                    record_hash TEXT UNIQUE,\n",
    "                    data JSON NOT NULL,\n",
    "                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            # Create pipeline runs table\n",
    "            conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS pipeline_runs (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    run_id TEXT UNIQUE NOT NULL,\n",
    "                    status TEXT NOT NULL,\n",
    "                    records_processed INTEGER,\n",
    "                    errors INTEGER,\n",
    "                    started_at TIMESTAMP,\n",
    "                    completed_at TIMESTAMP,\n",
    "                    details JSON\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get database connection with context manager\"\"\"\n",
    "        conn = sqlite3.connect(self.config.database_path)\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def store_raw_data(self, data: List[Dict[str, Any]], source: str) -> int:\n",
    "        \"\"\"Store raw data\"\"\"\n",
    "        stored_count = 0\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            for record in data:\n",
    "                record_hash = hashlib.md5(json.dumps(record, sort_keys=True).encode()).hexdigest()\n",
    "                try:\n",
    "                    conn.execute(\n",
    "                        'INSERT OR IGNORE INTO raw_data (data_source, record_hash, data) VALUES (?, ?, ?)',\n",
    "                        (source, record_hash, json.dumps(record))\n",
    "                    )\n",
    "                    stored_count += conn.rowcount\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error storing raw data: {str(e)}\")\n",
    "            \n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(f\"Stored {stored_count} raw records from {source}\")\n",
    "        return stored_count\n",
    "    \n",
    "    def store_processed_data(self, data: List[Dict[str, Any]], batch_id: str) -> int:\n",
    "        \"\"\"Store processed data\"\"\"\n",
    "        stored_count = 0\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            for record in data:\n",
    "                record_hash = record.get('_record_hash', 'unknown')\n",
    "                try:\n",
    "                    conn.execute(\n",
    "                        'INSERT OR REPLACE INTO processed_data (batch_id, record_hash, data) VALUES (?, ?, ?)',\n",
    "                        (batch_id, record_hash, json.dumps(record))\n",
    "                    )\n",
    "                    stored_count += conn.rowcount\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error storing processed data: {str(e)}\")\n",
    "            \n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(f\"Stored {stored_count} processed records for batch {batch_id}\")\n",
    "        return stored_count\n",
    "    \n",
    "    def get_latest_data(self, table: str, limit: int = 100) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get latest data from specified table\"\"\"\n",
    "        with self.get_connection() as conn:\n",
    "            cursor = conn.execute(\n",
    "                f'SELECT data FROM {table} ORDER BY id DESC LIMIT ?',\n",
    "                (limit,)\n",
    "            )\n",
    "            results = [json.loads(row[0]) for row in cursor.fetchall()]\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Pipeline Orchestrator\n",
    "class DataPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.storage = DataStorage(config)\n",
    "        self.transformer = DataTransformer()\n",
    "        self.sources = []\n",
    "        \n",
    "        # Create directories\n",
    "        Path(config.raw_data_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(config.processed_data_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def add_source(self, source: DataSource, name: str):\n",
    "        \"\"\"Add a data source to the pipeline\"\"\"\n",
    "        self.sources.append((source, name))\n",
    "        self.logger.info(f\"Added data source: {name}\")\n",
    "    \n",
    "    async def run_pipeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete pipeline\"\"\"\n",
    "        run_id = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        self.logger.info(f\"Starting pipeline run: {run_id}\")\n",
    "        \n",
    "        # Initialize run tracking\n",
    "        run_stats = {\n",
    "            'run_id': run_id,\n",
    "            'started_at': start_time.isoformat(),\n",
    "            'sources_processed': 0,\n",
    "            'total_records': 0,\n",
    "            'errors': 0,\n",
    "            'status': 'running'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Extract phase\n",
    "            all_raw_data = []\n",
    "            for source, source_name in self.sources:\n",
    "                try:\n",
    "                    self.logger.info(f\"Extracting from {source_name}\")\n",
    "                    data = await source.extract()\n",
    "                    \n",
    "                    # Store raw data\n",
    "                    stored_count = self.storage.store_raw_data(data, source_name)\n",
    "                    all_raw_data.extend(data)\n",
    "                    \n",
    "                    run_stats['sources_processed'] += 1\n",
    "                    run_stats['total_records'] += len(data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing source {source_name}: {str(e)}\")\n",
    "                    run_stats['errors'] += 1\n",
    "            \n",
    "            # Transform phase\n",
    "            self.logger.info(\"Starting transformation phase\")\n",
    "            cleaned_data = self.transformer.clean_data(all_raw_data)\n",
    "            \n",
    "            # Store processed data in batches\n",
    "            batch_size = self.config.batch_size\n",
    "            for i in range(0, len(cleaned_data), batch_size):\n",
    "                batch = cleaned_data[i:i + batch_size]\n",
    "                batch_id = f\"{run_id}_batch_{i // batch_size + 1}\"\n",
    "                self.storage.store_processed_data(batch, batch_id)\n",
    "            \n",
    "            # Export processed data\n",
    "            await self._export_processed_data(cleaned_data, run_id)\n",
    "            \n",
    "            # Update run stats\n",
    "            run_stats['status'] = 'completed'\n",
    "            run_stats['completed_at'] = datetime.now().isoformat()\n",
    "            run_stats['duration_seconds'] = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            self.logger.info(f\"Pipeline run {run_id} completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            run_stats['status'] = 'failed'\n",
    "            run_stats['error'] = str(e)\n",
    "            self.logger.error(f\"Pipeline run {run_id} failed: {str(e)}\")\n",
    "        \n",
    "        # Store run metadata\n",
    "        self._store_run_metadata(run_stats)\n",
    "        \n",
    "        return run_stats\n",
    "    \n",
    "    async def _export_processed_data(self, data: List[Dict[str, Any]], run_id: str):\n",
    "        \"\"\"Export processed data to files\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        # Export as CSV\n",
    "        df = pd.json_normalize(data)\n",
    "        csv_path = Path(self.config.processed_data_path) / f\"{run_id}_processed.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Export as JSON\n",
    "        json_path = Path(self.config.processed_data_path) / f\"{run_id}_processed.json\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Exported processed data to {csv_path} and {json_path}\")\n",
    "    \n",
    "    def _store_run_metadata(self, run_stats: Dict[str, Any]):\n",
    "        \"\"\"Store pipeline run metadata\"\"\"\n",
    "        with self.storage.get_connection() as conn:\n",
    "            conn.execute('''\n",
    "                INSERT OR REPLACE INTO pipeline_runs \n",
    "                (run_id, status, records_processed, errors, started_at, completed_at, details)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                run_stats['run_id'],\n",
    "                run_stats['status'],\n",
    "                run_stats.get('total_records', 0),\n",
    "                run_stats.get('errors', 0),\n",
    "                run_stats.get('started_at'),\n",
    "                run_stats.get('completed_at'),\n",
    "                json.dumps(run_stats)\n",
    "            ))\n",
    "            conn.commit()\n",
    "    \n",
    "    def get_pipeline_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get pipeline status and statistics\"\"\"\n",
    "        with self.storage.get_connection() as conn:\n",
    "            # Get recent runs\n",
    "            cursor = conn.execute('''\n",
    "                SELECT * FROM pipeline_runs \n",
    "                ORDER BY started_at DESC \n",
    "                LIMIT 10\n",
    "            ''')\n",
    "            \n",
    "            runs = []\n",
    "            for row in cursor.fetchall():\n",
    "                runs.append({\n",
    "                    'run_id': row[1],\n",
    "                    'status': row[2],\n",
    "                    'records_processed': row[3],\n",
    "                    'errors': row[4],\n",
    "                    'started_at': row[5],\n",
    "                    'completed_at': row[6]\n",
    "                })\n",
    "            \n",
    "            # Get data statistics\n",
    "            raw_count = conn.execute('SELECT COUNT(*) FROM raw_data').fetchone()[0]\n",
    "            processed_count = conn.execute('SELECT COUNT(*) FROM processed_data').fetchone()[0]\n",
    "        \n",
    "        return {\n",
    "            'recent_runs': runs,\n",
    "            'total_raw_records': raw_count,\n",
    "            'total_processed_records': processed_count,\n",
    "            'last_run': runs[0] if runs else None\n",
    "        }\n",
    "\n",
    "# Scheduler and Monitoring\n",
    "class PipelineScheduler:\n",
    "    \"\"\"Handle pipeline scheduling and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: DataPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.is_running = False\n",
    "    \n",
    "    def schedule_daily(self, time_str: str = \"02:00\"):\n",
    "        \"\"\"Schedule pipeline to run daily\"\"\"\n",
    "        schedule.every().day.at(time_str).do(self._run_scheduled_pipeline)\n",
    "        self.logger.info(f\"Scheduled daily pipeline run at {time_str}\")\n",
    "    \n",
    "    def schedule_hourly(self):\n",
    "        \"\"\"Schedule pipeline to run hourly\"\"\"\n",
    "        schedule.every().hour.do(self._run_scheduled_pipeline)\n",
    "        self.logger.info(\"Scheduled hourly pipeline runs\")\n",
    "    \n",
    "    def _run_scheduled_pipeline(self):\n",
    "        \"\"\"Run pipeline in scheduled mode\"\"\"\n",
    "        if self.is_running:\n",
    "            self.logger.warning(\"Pipeline already running, skipping scheduled run\")\n",
    "            return\n",
    "        \n",
    "        self.is_running = True\n",
    "        try:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            result = loop.run_until_complete(self.pipeline.run_pipeline())\n",
    "            self.logger.info(f\"Scheduled pipeline run completed: {result['status']}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Scheduled pipeline run failed: {str(e)}\")\n",
    "        finally:\n",
    "            self.is_running = False\n",
    "    \n",
    "    def start_scheduler(self):\n",
    "        \"\"\"Start the scheduler\"\"\"\n",
    "        self.logger.info(\"Starting pipeline scheduler\")\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute\n",
    "\n",
    "# Example usage and demo\n",
    "async def demo_pipeline():\n",
    "    \"\"\"Demonstrate the pipeline with sample data\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    config = PipelineConfig()\n",
    "    logger = setup_logging(config.log_path)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = DataPipeline(config)\n",
    "    \n",
    "    # Add sample data sources\n",
    "    # Note: These are examples - replace with your actual data sources\n",
    "    \n",
    "    # Example API source (using a mock data API)\n",
    "    api_source = APIDataSource(\n",
    "        base_url=\"https://jsonplaceholder.typicode.com\",\n",
    "        endpoints=[\"users\", \"posts\"]\n",
    "    )\n",
    "    pipeline.add_source(api_source, \"sample_api\")\n",
    "    \n",
    "    # Example file source (create sample data if needed)\n",
    "    sample_data = [\n",
    "        {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\", \"signup_date\": \"2024-01-15\"},\n",
    "        {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\", \"signup_date\": \"2024-02-20\"},\n",
    "        {\"id\": 3, \"name\": \"Bob Johnson\", \"email\": \"bob@example.com\", \"signup_date\": \"2024-03-10\"}\n",
    "    ]\n",
    "    \n",
    "    # Create sample file\n",
    "    sample_file_path = Path(config.raw_data_path) / \"sample_users.json\"\n",
    "    with open(sample_file_path, 'w') as f:\n",
    "        json.dump(sample_data, f)\n",
    "    \n",
    "    file_source = FileDataSource([str(sample_file_path)])\n",
    "    pipeline.add_source(file_source, \"sample_file\")\n",
    "    \n",
    "    # Run pipeline\n",
    "    logger.info(\"Running demo pipeline\")\n",
    "    result = await pipeline.run_pipeline()\n",
    "    \n",
    "    # Display results\n",
    "    logger.info(\"Pipeline Results:\")\n",
    "    logger.info(f\"Status: {result['status']}\")\n",
    "    logger.info(f\"Total Records: {result['total_records']}\")\n",
    "    logger.info(f\"Sources Processed: {result['sources_processed']}\")\n",
    "    logger.info(f\"Errors: {result['errors']}\")\n",
    "    \n",
    "    # Show pipeline status\n",
    "    status = pipeline.get_pipeline_status()\n",
    "    logger.info(\"Pipeline Status:\")\n",
    "    logger.info(f\"Total Raw Records: {status['total_raw_records']}\")\n",
    "    logger.info(f\"Total Processed Records: {status['total_processed_records']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the demo\n",
    "    result = asyncio.run(demo_pipeline())\n",
    "    \n",
    "    # Example of setting up scheduler (commented out for demo)\n",
    "    # config = PipelineConfig()\n",
    "    # pipeline = DataPipeline(config)\n",
    "    # scheduler = PipelineScheduler(pipeline)\n",
    "    # scheduler.schedule_daily(\"02:00\")\n",
    "    # scheduler.start_scheduler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
