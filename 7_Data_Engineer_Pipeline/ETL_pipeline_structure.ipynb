{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac77efb-12d5-4619-af18-570d44056f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete ETL Pipeline Example\n",
    "============================\n",
    "\n",
    "This example demonstrates a production-ready ETL pipeline that:\n",
    "- Extracts data from multiple sources (CSV, API, Database)\n",
    "- Transforms data with validation and cleaning\n",
    "- Loads data to a target database\n",
    "- Includes error handling, logging, and data quality checks\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class ETLConfig:\n",
    "    \"\"\"Configuration class for ETL pipeline\"\"\"\n",
    "    source_csv_path: str = \"data/sales_data.csv\"\n",
    "    source_api_url: str = \"https://jsonplaceholder.typicode.com/users\"\n",
    "    source_db_path: str = \"data/source.db\"\n",
    "    target_db_path: str = \"data/warehouse.db\"\n",
    "    log_file: str = \"logs/etl_pipeline.log\"\n",
    "    batch_size: int = 1000\n",
    "    max_retries: int = 3\n",
    "    timeout: int = 30\n",
    "\n",
    "class ETLLogger:\n",
    "    \"\"\"Custom logging setup for ETL pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file: str, level: str = \"INFO\"):\n",
    "        self.logger = logging.getLogger(\"ETL_Pipeline\")\n",
    "        self.logger.setLevel(getattr(logging, level))\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_formatter = logging.Formatter(\n",
    "            '%(levelname)s - %(message)s'\n",
    "        )\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        \n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "    \n",
    "    def get_logger(self):\n",
    "        return self.logger\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Data quality validation and checks\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.quality_report = {}\n",
    "    \n",
    "    def check_null_values(self, df: pd.DataFrame, columns: List[str]) -> Dict:\n",
    "        \"\"\"Check for null values in specified columns\"\"\"\n",
    "        null_report = {}\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                null_count = df[col].isnull().sum()\n",
    "                null_percentage = (null_count / len(df)) * 100\n",
    "                null_report[col] = {\n",
    "                    'null_count': null_count,\n",
    "                    'null_percentage': round(null_percentage, 2)\n",
    "                }\n",
    "        return null_report\n",
    "    \n",
    "    def check_duplicates(self, df: pd.DataFrame, key_columns: List[str]) -> int:\n",
    "        \"\"\"Check for duplicate records based on key columns\"\"\"\n",
    "        return df.duplicated(subset=key_columns).sum()\n",
    "    \n",
    "    def check_data_types(self, df: pd.DataFrame, expected_types: Dict) -> Dict:\n",
    "        \"\"\"Validate data types match expectations\"\"\"\n",
    "        type_issues = {}\n",
    "        for col, expected_type in expected_types.items():\n",
    "            if col in df.columns:\n",
    "                actual_type = str(df[col].dtype)\n",
    "                if expected_type not in actual_type:\n",
    "                    type_issues[col] = {\n",
    "                        'expected': expected_type,\n",
    "                        'actual': actual_type\n",
    "                    }\n",
    "        return type_issues\n",
    "    \n",
    "    def validate_data(self, df: pd.DataFrame, validation_rules: Dict) -> bool:\n",
    "        \"\"\"Run comprehensive data validation\"\"\"\n",
    "        self.logger.info(f\"Running data quality checks on {len(df)} records\")\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_cols = validation_rules.get('required_columns', [])\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            self.logger.error(f\"Missing required columns: {missing_cols}\")\n",
    "            return False\n",
    "        \n",
    "        # Check null values\n",
    "        null_report = self.check_null_values(df, required_cols)\n",
    "        self.quality_report['null_values'] = null_report\n",
    "        \n",
    "        # Check duplicates\n",
    "        key_cols = validation_rules.get('key_columns', [])\n",
    "        if key_cols:\n",
    "            duplicate_count = self.check_duplicates(df, key_cols)\n",
    "            self.quality_report['duplicates'] = duplicate_count\n",
    "            if duplicate_count > 0:\n",
    "                self.logger.warning(f\"Found {duplicate_count} duplicate records\")\n",
    "        \n",
    "        # Check data types\n",
    "        expected_types = validation_rules.get('data_types', {})\n",
    "        type_issues = self.check_data_types(df, expected_types)\n",
    "        self.quality_report['type_issues'] = type_issues\n",
    "        \n",
    "        # Log quality report\n",
    "        self.logger.info(f\"Data quality report: {json.dumps(self.quality_report, indent=2)}\")\n",
    "        \n",
    "        return len(type_issues) == 0\n",
    "\n",
    "class DataExtractor:\n",
    "    \"\"\"Handle data extraction from various sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ETLConfig, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "    \n",
    "    def extract_from_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from CSV file\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extracting data from CSV: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            self.logger.info(f\"Successfully extracted {len(df)} records from CSV\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"CSV file not found: {file_path}\")\n",
    "            # Create sample data for demo\n",
    "            return self._create_sample_sales_data()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting from CSV: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_from_api(self, url: str, retries: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from REST API with retry logic\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                self.logger.info(f\"Extracting data from API: {url} (attempt {attempt + 1})\")\n",
    "                response = requests.get(url, timeout=self.config.timeout)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                df = pd.json_normalize(data)\n",
    "                self.logger.info(f\"Successfully extracted {len(df)} records from API\")\n",
    "                return df\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.warning(f\"API request failed (attempt {attempt + 1}): {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    self.logger.error(\"All API retry attempts failed\")\n",
    "                    return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    def extract_from_database(self, db_path: str, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data from SQLite database\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Extracting data from database: {db_path}\")\n",
    "            with sqlite3.connect(db_path) as conn:\n",
    "                df = pd.read_sql_query(query, conn)\n",
    "            self.logger.info(f\"Successfully extracted {len(df)} records from database\")\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            self.logger.error(f\"Database extraction error: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error in database extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_sample_sales_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Create sample sales data for demonstration\"\"\"\n",
    "        self.logger.info(\"Creating sample sales data for demonstration\")\n",
    "        sample_data = {\n",
    "            'transaction_id': range(1, 101),\n",
    "            'customer_id': [f\"CUST_{i:03d}\" for i in range(1, 101)],\n",
    "            'product_id': [f\"PROD_{i % 10 + 1:03d}\" for i in range(100)],\n",
    "            'quantity': [i % 5 + 1 for i in range(100)],\n",
    "            'unit_price': [10.0 + (i % 50) for i in range(100)],\n",
    "            'transaction_date': [\n",
    "                (datetime.now() - timedelta(days=i % 30)).strftime('%Y-%m-%d')\n",
    "                for i in range(100)\n",
    "            ]\n",
    "        }\n",
    "        return pd.DataFrame(sample_data)\n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"Handle data transformation and cleaning\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def clean_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and transform sales data\"\"\"\n",
    "        self.logger.info(\"Starting sales data transformation\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Remove duplicates\n",
    "        initial_count = len(df_clean)\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        duplicates_removed = initial_count - len(df_clean)\n",
    "        if duplicates_removed > 0:\n",
    "            self.logger.info(f\"Removed {duplicates_removed} duplicate records\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_clean = df_clean.dropna(subset=['transaction_id', 'customer_id'])\n",
    "        \n",
    "        # Data type conversions\n",
    "        if 'transaction_date' in df_clean.columns:\n",
    "            df_clean['transaction_date'] = pd.to_datetime(df_clean['transaction_date'])\n",
    "        \n",
    "        # Calculate derived fields\n",
    "        if 'quantity' in df_clean.columns and 'unit_price' in df_clean.columns:\n",
    "            df_clean['total_amount'] = df_clean['quantity'] * df_clean['unit_price']\n",
    "        \n",
    "        # Add audit fields\n",
    "        df_clean['processed_at'] = datetime.now()\n",
    "        df_clean['record_hash'] = df_clean.apply(\n",
    "            lambda row: hashlib.md5(str(row.to_dict()).encode()).hexdigest(), axis=1\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Transformation complete: {len(df_clean)} records processed\")\n",
    "        return df_clean\n",
    "    \n",
    "    def transform_user_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform user data from API\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        self.logger.info(\"Starting user data transformation\")\n",
    "        \n",
    "        df_transformed = df.copy()\n",
    "        \n",
    "        # Flatten nested structures\n",
    "        if 'address.geo.lat' in df_transformed.columns:\n",
    "            df_transformed['latitude'] = pd.to_numeric(df_transformed['address.geo.lat'], errors='coerce')\n",
    "            df_transformed['longitude'] = pd.to_numeric(df_transformed['address.geo.lng'], errors='coerce')\n",
    "        \n",
    "        # Create full address\n",
    "        address_cols = ['address.street', 'address.city', 'address.zipcode']\n",
    "        available_cols = [col for col in address_cols if col in df_transformed.columns]\n",
    "        if available_cols:\n",
    "            df_transformed['full_address'] = df_transformed[available_cols].fillna('').agg(' '.join, axis=1)\n",
    "        \n",
    "        # Clean and standardize data\n",
    "        if 'email' in df_transformed.columns:\n",
    "            df_transformed['email'] = df_transformed['email'].str.lower().str.strip()\n",
    "        \n",
    "        # Add audit fields\n",
    "        df_transformed['extracted_at'] = datetime.now()\n",
    "        \n",
    "        self.logger.info(f\"User data transformation complete: {len(df_transformed)} records\")\n",
    "        return df_transformed\n",
    "    \n",
    "    def aggregate_sales_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create sales summary aggregations\"\"\"\n",
    "        if df.empty or 'total_amount' not in df.columns:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        self.logger.info(\"Creating sales summary aggregations\")\n",
    "        \n",
    "        # Daily sales summary\n",
    "        daily_summary = df.groupby('transaction_date').agg({\n",
    "            'total_amount': ['sum', 'count', 'mean'],\n",
    "            'quantity': 'sum'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten column names\n",
    "        daily_summary.columns = ['_'.join(col).strip() for col in daily_summary.columns]\n",
    "        daily_summary = daily_summary.reset_index()\n",
    "        daily_summary['summary_type'] = 'daily'\n",
    "        daily_summary['created_at'] = datetime.now()\n",
    "        \n",
    "        self.logger.info(f\"Created daily summary with {len(daily_summary)} records\")\n",
    "        return daily_summary\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Handle data loading to target systems\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ETLConfig, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "    \n",
    "    def setup_target_database(self):\n",
    "        \"\"\"Create target database tables\"\"\"\n",
    "        self.logger.info(\"Setting up target database schema\")\n",
    "        \n",
    "        # Create data directory if it doesn't exist\n",
    "        Path(self.config.target_db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with sqlite3.connect(self.config.target_db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Sales data table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS sales_data (\n",
    "                    transaction_id INTEGER PRIMARY KEY,\n",
    "                    customer_id TEXT NOT NULL,\n",
    "                    product_id TEXT,\n",
    "                    quantity INTEGER,\n",
    "                    unit_price REAL,\n",
    "                    total_amount REAL,\n",
    "                    transaction_date DATE,\n",
    "                    processed_at TIMESTAMP,\n",
    "                    record_hash TEXT\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # User data table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS user_data (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    name TEXT,\n",
    "                    username TEXT,\n",
    "                    email TEXT,\n",
    "                    phone TEXT,\n",
    "                    website TEXT,\n",
    "                    full_address TEXT,\n",
    "                    latitude REAL,\n",
    "                    longitude REAL,\n",
    "                    extracted_at TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Sales summary table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS sales_summary (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    transaction_date DATE,\n",
    "                    total_amount_sum REAL,\n",
    "                    total_amount_count INTEGER,\n",
    "                    total_amount_mean REAL,\n",
    "                    quantity_sum INTEGER,\n",
    "                    summary_type TEXT,\n",
    "                    created_at TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.commit()\n",
    "            self.logger.info(\"Target database schema created successfully\")\n",
    "    \n",
    "    def load_data(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append') -> bool:\n",
    "        \"\"\"Load data to target database\"\"\"\n",
    "        if df.empty:\n",
    "            self.logger.warning(f\"No data to load for table {table_name}\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(f\"Loading {len(df)} records to {table_name}\")\n",
    "            \n",
    "            with sqlite3.connect(self.config.target_db_path) as conn:\n",
    "                df.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "            \n",
    "            self.logger.info(f\"Successfully loaded data to {table_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data to {table_name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_load_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about loaded data\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.config.target_db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                stats = {}\n",
    "                tables = ['sales_data', 'user_data', 'sales_summary']\n",
    "                \n",
    "                for table in tables:\n",
    "                    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                    count = cursor.fetchone()[0]\n",
    "                    stats[table] = count\n",
    "                \n",
    "                return stats\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting load statistics: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"Main ETL Pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ETLConfig = None):\n",
    "        self.config = config or ETLConfig()\n",
    "        self.logger_setup = ETLLogger(self.config.log_file)\n",
    "        self.logger = self.logger_setup.get_logger()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.extractor = DataExtractor(self.config, self.logger)\n",
    "        self.transformer = DataTransformer(self.logger)\n",
    "        self.loader = DataLoader(self.config, self.logger)\n",
    "        self.quality_checker = DataQualityChecker(self.logger)\n",
    "        \n",
    "        self.pipeline_stats = {\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'records_processed': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self) -> bool:\n",
    "        \"\"\"Execute the complete ETL pipeline\"\"\"\n",
    "        self.pipeline_stats['start_time'] = datetime.now()\n",
    "        self.logger.info(\"=\"*50)\n",
    "        self.logger.info(\"Starting ETL Pipeline\")\n",
    "        self.logger.info(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Setup target database\n",
    "            self.loader.setup_target_database()\n",
    "            \n",
    "            # Extract data from multiple sources\n",
    "            sales_data = self.extractor.extract_from_csv(self.config.source_csv_path)\n",
    "            user_data = self.extractor.extract_from_api(self.config.source_api_url)\n",
    "            \n",
    "            # Data quality validation\n",
    "            sales_validation_rules = {\n",
    "                'required_columns': ['transaction_id', 'customer_id'],\n",
    "                'key_columns': ['transaction_id'],\n",
    "                'data_types': {'transaction_id': 'int', 'customer_id': 'object'}\n",
    "            }\n",
    "            \n",
    "            if not self.quality_checker.validate_data(sales_data, sales_validation_rules):\n",
    "                self.logger.error(\"Sales data quality validation failed\")\n",
    "                return False\n",
    "            \n",
    "            # Transform data\n",
    "            sales_clean = self.transformer.clean_sales_data(sales_data)\n",
    "            user_clean = self.transformer.transform_user_data(user_data)\n",
    "            sales_summary = self.transformer.aggregate_sales_summary(sales_clean)\n",
    "            \n",
    "            # Load data\n",
    "            success = True\n",
    "            success &= self.loader.load_data(sales_clean, 'sales_data', 'replace')\n",
    "            success &= self.loader.load_data(user_clean, 'user_data', 'replace')\n",
    "            success &= self.loader.load_data(sales_summary, 'sales_summary', 'replace')\n",
    "            \n",
    "            if not success:\n",
    "                self.logger.error(\"Data loading failed\")\n",
    "                return False\n",
    "            \n",
    "            # Get final statistics\n",
    "            load_stats = self.loader.get_load_statistics()\n",
    "            self.pipeline_stats['records_processed'] = sum(load_stats.values())\n",
    "            \n",
    "            self.pipeline_stats['end_time'] = datetime.now()\n",
    "            duration = self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']\n",
    "            \n",
    "            self.logger.info(\"=\"*50)\n",
    "            self.logger.info(\"ETL Pipeline Completed Successfully\")\n",
    "            self.logger.info(f\"Duration: {duration}\")\n",
    "            self.logger.info(f\"Records processed: {self.pipeline_stats['records_processed']}\")\n",
    "            self.logger.info(f\"Load statistics: {load_stats}\")\n",
    "            self.logger.info(\"=\"*50)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "            self.pipeline_stats['errors'].append(str(e))\n",
    "            return False\n",
    "    \n",
    "    def get_pipeline_stats(self) -> Dict:\n",
    "        \"\"\"Get pipeline execution statistics\"\"\"\n",
    "        return self.pipeline_stats\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create ETL pipeline instance\n",
    "    pipeline = ETLPipeline()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    success = pipeline.run_pipeline()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ ETL Pipeline executed successfully!\")\n",
    "        stats = pipeline.get_pipeline_stats()\n",
    "        print(f\"Records processed: {stats['records_processed']}\")\n",
    "        print(f\"Execution time: {stats['end_time'] - stats['start_time']}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ETL Pipeline failed!\")\n",
    "        print(\"Check logs for details.\")\n",
    "    \n",
    "    # Example of how to query the results\n",
    "    import sqlite3\n",
    "    \n",
    "    print(\"\\nüìä Sample Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    config = ETLConfig()\n",
    "    try:\n",
    "        with sqlite3.connect(config.target_db_path) as conn:\n",
    "            # Show sales summary\n",
    "            df_summary = pd.read_sql_query(\n",
    "                \"SELECT * FROM sales_summary ORDER BY transaction_date DESC LIMIT 5\", \n",
    "                conn\n",
    "            )\n",
    "            print(\"Sales Summary (Last 5 days):\")\n",
    "            print(df_summary.to_string(index=False))\n",
    "            \n",
    "            # Show top customers by total amount\n",
    "            df_customers = pd.read_sql_query(\"\"\"\n",
    "                SELECT customer_id, \n",
    "                       COUNT(*) as transaction_count,\n",
    "                       SUM(total_amount) as total_spent\n",
    "                FROM sales_data \n",
    "                GROUP BY customer_id \n",
    "                ORDER BY total_spent DESC \n",
    "                LIMIT 5\n",
    "            \"\"\", conn)\n",
    "            print(\"\\n\\nTop 5 Customers by Total Spent:\")\n",
    "            print(df_customers.to_string(index=False))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a509a43-33b8-4cb0-8ae8-8f849f4b3a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda5867-3ef7-4131-b1ec-e3a6cf1d4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ETL pipeline structure\n",
    "class ETLPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = setup_logging()\n",
    "    \n",
    "    def extract(self, source):\n",
    "        \"\"\"Extract data from source systems\"\"\"\n",
    "        try:\n",
    "            data = self.connect_and_extract(source)\n",
    "            self.logger.info(f\"Extracted {len(data)} records\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.handle_error(\"Extract\", e)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"Apply transformations to data\"\"\"\n",
    "        try:\n",
    "            cleaned_data = self.clean_data(data)\n",
    "            validated_data = self.validate_data(cleaned_data)\n",
    "            enriched_data = self.enrich_data(validated_data)\n",
    "            return enriched_data\n",
    "        except Exception as e:\n",
    "            self.handle_error(\"Transform\", e)\n",
    "    \n",
    "    def load(self, data, target):\n",
    "        \"\"\"Load data to target system\"\"\"\n",
    "        try:\n",
    "            self.connect_and_load(data, target)\n",
    "            self.logger.info(f\"Loaded {len(data)} records\")\n",
    "        except Exception as e:\n",
    "            self.handle_error(\"Load\", e)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the full ETL pipeline\"\"\"\n",
    "        data = self.extract(self.config.source)\n",
    "        transformed_data = self.transform(data)\n",
    "        self.load(transformed_data, self.config.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
